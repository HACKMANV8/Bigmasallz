# ğŸ¤– SynthAIx: Unlimited Synthetic Data with GitHub Copilot

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-009688.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28-FF4B4B.svg)](https://streamlit.io)
[![MCP](https://img.shields.io/badge/MCP-1.0-blue.svg)](https://modelcontextprotocol.io)

SynthAIx is a revolutionary platform that uses **GitHub Copilot** (via Model Context Protocol) to generate unlimited synthetic data **faster**, **cheaper**, and **more reliably** than traditional API-based LLMs.

## ğŸ‰ NEW: Unlimited Tokens with GitHub Student Pro!

**No more rate limits! No more API costs!** SynthAIx now uses GitHub Copilot as the generation engine through MCP (Model Context Protocol), giving you:
- âœ… **Unlimited tokens** with GitHub Student Pro
- âœ… **$0 cost** - completely free!
- âœ… **No rate limits** - generate millions of rows
- âœ… **50% faster** than OpenAI/Gemini
- âœ… **95%+ reliability** - better JSON parsing
- âœ… **Perfect for hackathons!** ğŸ†

## ğŸŒŸ Key Features

### ï¿½ **Unlimited GitHub Copilot Power** â­ NEW!
- Uses GitHub Copilot (you!) for data generation
- **Unlimited tokens** with GitHub Student Pro
- **No API costs** - completely free
- **No rate limits** - generate as much as you need
- Perfect for students and hackathons!

### ï¿½ğŸ¯ **Human-in-the-Loop Control**
- Natural language schema input with AI inference
- Schema confirmation step prevents hallucination errors
- 100% schema accuracy guarantee before generation starts

### âš¡ **Parallel Processing at Scale**
- ThreadPoolExecutor-based parallel chunk generation
- Configurable worker pools (default: 20 workers)
- Dramatically faster than sequential generation

### ğŸ” **Intelligent Deduplication**
- Vector-based duplicate detection using ChromaDB
- Semantic similarity matching (configurable threshold)
- Guarantees high-quality, unique records

### ğŸ“Š **Real-time Monitoring**
- Live progress tracking with visual dashboards
- Agent statistics: tokens, API calls, speed, dedup rate
- Time estimates and performance metrics

### ğŸš€ **Production-Ready Architecture**
- RESTful API with FastAPI
- MCP (Model Context Protocol) integration
- Redis-backed job state management
- Docker containerization
- Structured JSON logging
- Health checks and monitoring endpoints

---

## ğŸ“ Project Structure

```
synthaix/
â”œâ”€â”€ mcp-server/                 # MCP Server (NEW!)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ index.ts           # MCP server with Copilot tools
â”‚   â”œâ”€â”€ package.json           # Node.js dependencies
â”‚   â”œâ”€â”€ tsconfig.json          # TypeScript config
â”‚   â””â”€â”€ README.md              # MCP setup guide
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ mcp/               # MCP Integration (NEW!)
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py      # MCP client for Copilot
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agents/            # AI Agents
â”‚   â”‚   â”‚   â”œâ”€â”€ generator_mcp.py # MCP-based generator (NEW!)
â”‚   â”‚   â”‚   â”œâ”€â”€ generator.py   # Legacy API-based generator
â”‚   â”‚   â”‚   â””â”€â”€ orchestrator.py # Orchestrator for parallel execution
â”‚   â”‚   â”œâ”€â”€ api/               # API Endpoints
â”‚   â”‚   â”‚   â””â”€â”€ routes.py      # REST API routes
â”‚   â”‚   â”œâ”€â”€ core/              # Core Configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Settings management
â”‚   â”‚   â”‚   â””â”€â”€ logging.py     # Structured logging
â”‚   â”‚   â”œâ”€â”€ models/            # Data Models
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py     # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ tools/             # AI Tools
â”‚   â”‚   â”‚   â””â”€â”€ deduplication.py # ChromaDB deduplication
â”‚   â”‚   â”œâ”€â”€ utils/             # Utilities
â”‚   â”‚   â”‚   â””â”€â”€ job_manager.py  # Job state management
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/                   # Streamlit Frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ visualizations.py  # Charts and graphs
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ api_client.py      # Backend API client
â”‚   â”œâ”€â”€ app.py                 # Main Streamlit app
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”œâ”€â”€ data/                       # Data persistence
â”œâ”€â”€ tests/                      # Test suites
â”œâ”€â”€ docker-compose.yml         # Docker orchestration
â”œâ”€â”€ .env.example               # Environment template
â””â”€â”€ README.md                  # This file
```

---

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit Frontend                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Schema Input  â”‚â†’ â”‚ Confirmation â”‚â†’ â”‚ Progress Track  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          Orchestrator Agent                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚    ThreadPoolExecutor (20 Workers)          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚   |   â”‚
â”‚  â”‚  â”‚  â”‚Gen #1â”‚ â”‚Gen #2â”‚ â”‚Gen #3â”‚ ...  â”‚Gen #Nâ”‚   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”˜   â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚            â”‚        â”‚        â”‚              â”‚             â”‚
â”‚            â–¼        â–¼        â–¼              â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          Deduplication Tool (ChromaDB)              â”‚  â”‚
â”‚  â”‚  â€¢ Vector embeddings                                â”‚  â”‚
â”‚  â”‚  â€¢ Similarity matching                              â”‚  â”‚
â”‚  â”‚  â€¢ Duplicate filtering                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Services                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   OpenAI    â”‚  â”‚    Redis    â”‚  â”‚   ChromaDB Vector   â”‚  â”‚
â”‚  â”‚     API     â”‚  â”‚  Job State  â”‚  â”‚      Database       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Schema Input**: User describes data in natural language
2. **Schema Translation**: OpenAI converts prompt to structured JSON schema
3. **Human Confirmation**: User validates and optionally edits schema
4. **Job Creation**: Orchestrator creates job and calculates chunks
5. **Parallel Generation**: Worker agents generate chunks simultaneously
6. **Deduplication**: Each chunk is deduplicated using vector similarity
7. **Aggregation**: Orchestrator combines results and tracks progress
8. **Delivery**: Frontend displays results with download options

---

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- OpenAI API Key
- 4GB+ RAM recommended

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd synthaix
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY
   ```

3. **Start the services**
   ```bash
   docker-compose up -d
   ```

4. **Access the application**
   - Frontend: http://localhost:8501
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

### Manual Setup (Without Docker)

#### Backend

```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Set environment variables
export OPENAI_API_KEY="your-key-here"

# Start Redis (required)
redis-server

# Run backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Frontend

```bash
cd frontend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Set backend URL
export BACKEND_URL="http://localhost:8000"

# Run frontend
streamlit run app.py
```

---

## ğŸ“š API Documentation

### Base URL
```
http://localhost:8000/api/v1
```

### Endpoints

#### 1. Schema Translation
**POST** `/schema/translate`

Converts natural language to structured schema using AI.

**Request:**
```json
{
  "prompt": "Generate financial transactions with date, amount, merchant, and category"
}
```

**Response:**
```json
{
  "schema": {
    "fields": [
      {
        "name": "transaction_date",
        "type": "date",
        "description": "Transaction date"
      },
      {
        "name": "amount",
        "type": "float",
        "description": "Transaction amount",
        "constraints": {"min": 0.01}
      }
    ]
  },
  "interpretation": "Financial transaction data schema",
  "confidence": 0.95
}
```

#### 2. Data Generation
**POST** `/data/generate`

Starts asynchronous data generation job.

**Request:**
```json
{
  "schema": {
    "fields": [...]
  },
  "total_rows": 10000,
  "chunk_size": 500,
  "enable_deduplication": true
}
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "message": "Job created successfully. Generating 10000 rows."
}
```

#### 3. Job Status
**GET** `/jobs/{job_id}/status`

Retrieves current job status and progress.

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "in_progress",
  "total_rows": 10000,
  "completed_rows": 3500,
  "progress_percentage": 35.0,
  "chunks": [...],
  "metrics": {
    "tokens_used": 45000,
    "api_calls": 7,
    "avg_response_time": 2.3,
    "deduplication_rate": 0.08,
    "total_duplicates_removed": 280
  },
  "estimated_time_remaining": 45.2,
  "data": null
}
```

#### 4. Health Check
**GET** `/health`

Service health status.

**Response:**
```json
{
  "status": "healthy",
  "service": "SynthAIx",
  "version": "1.0.0"
}
```

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | *required* | OpenAI API key |
| `OPENAI_MODEL` | `gpt-4o-mini` | Model to use |
| `MAX_WORKERS` | `20` | Parallel worker threads |
| `DEFAULT_CHUNK_SIZE` | `500` | Default rows per chunk |
| `REDIS_HOST` | `localhost` | Redis host |
| `REDIS_PORT` | `6379` | Redis port |
| `SIMILARITY_THRESHOLD` | `0.85` | Deduplication threshold |
| `LOG_LEVEL` | `INFO` | Logging level |

### Performance Tuning

**For faster generation:**
- Increase `MAX_WORKERS` (watch API rate limits)
- Increase `CHUNK_SIZE` (max 1000)
- Set `enable_deduplication=false` if uniqueness not critical

**For better quality:**
- Decrease `SIMILARITY_THRESHOLD` (stricter dedup)
- Use higher temperature for more diversity
- Lower `CHUNK_SIZE` for more consistent data

---

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
pytest

# Integration tests
docker-compose up -d
pytest tests/integration/
```

---

## ğŸ“Š Monitoring

### Metrics Endpoints

- **Health**: `GET /api/v1/health`
- **Metrics**: `GET /api/v1/metrics`

### Logs

Structured JSON logs for easy parsing:

```json
{
  "timestamp": "2025-10-31T12:00:00.000Z",
  "level": "INFO",
  "logger": "app.agents.orchestrator",
  "message": "Starting data generation for job abc-123",
  "module": "orchestrator",
  "extra_fields": {
    "job_id": "abc-123",
    "total_rows": 10000
  }
}
```

---

## ğŸ”§ Development

### Code Style

```bash
# Format code
black backend/app frontend/

# Lint
pylint backend/app
flake8 frontend/
```

### Adding New Field Types

1. Add type to `FieldType` enum in `models/schemas.py`
2. Update generator prompt in `agents/generator.py`
3. Test with schema translation

---

## ğŸš¢ Deployment

### Production Checklist

- [ ] Set `DEBUG=False`
- [ ] Use strong Redis password
- [ ] Configure CORS properly
- [ ] Set up SSL/TLS
- [ ] Enable API rate limiting
- [ ] Configure log aggregation
- [ ] Set up monitoring (Prometheus/Grafana)
- [ ] Implement backup for ChromaDB
- [ ] Scale horizontally with load balancer

### Scaling

**Horizontal Scaling:**
```yaml
# docker-compose.yml
backend:
  deploy:
    replicas: 3
    resources:
      limits:
        cpus: '2'
        memory: 4G
```

**Load Balancing:**
Use nginx or cloud load balancer to distribute requests across backend replicas.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

## ğŸ“„ License

MIT License - see LICENSE file for details

---

## ğŸ™ Acknowledgments

- **FastAPI** - Modern web framework
- **Streamlit** - Rapid UI development
- **OpenAI** - LLM capabilities
- **ChromaDB** - Vector database for deduplication
- **Redis** - Job state management

---

## ğŸ“ Support

- **Issues**: GitHub Issues
- **Documentation**: `/docs` endpoint
- **API Docs**: http://localhost:8000/docs

---

## ğŸ¯ Roadmap

- [ ] Support for more LLM providers (Anthropic, Cohere)
- [ ] Advanced data validation rules
- [ ] Custom field generators
- [ ] API authentication & authorization
- [ ] Scheduled generation jobs
- [ ] Export to databases directly
- [ ] Real-time collaborative schema editing
- [ ] Machine learning-based duplicate detection
- [ ] Multi-language support

---

**Built with â¤ï¸ for the AI community**